/**
  * A k-means clustering org.apache.spark.apache.org/docs/latest/mllib-clustering.html#k-means implementation
  * that clusters feature vectors into a clusters
  * The input file to the program is of the following CSV format
  *
  * |-----------+-------------------------+----------
  * |SubjectID | ... feature vectors ... | SegmentID
  * |-----------+-------------------------+----------
  *
  * In data cleansing step header, the first and the last columns are dropped,
  * The feature vectors can be expanded and collapsed as desired but a minimum one is required.
  *
  */

package com.behzad.cs.kmeans

import com.behzad.cs.{AbstractParams, MLUtil}
import MLUtil._
import org.apache.spark.mllib.clustering._
import org.apache.spark.{SparkConf, SparkContext}
import scopt.OptionParser

/**
  * Created by Behzad Altaf
  */
object KMeansPrediction {

  object InitializationMode extends Enumeration {
    type InitializationMode = Value
    val Random, Parallel = Value
  }

  import InitializationMode._

  case class Params(
                     inputFile: String = null,
                     outputFile: String = null,
                     initializationMode: InitializationMode = Parallel,
                     seed: Long = scala.util.Random.nextLong(),
                     master: String = "local",
                     appName: String = "Subject Segmentation using k-means",
                     runs: Int = 10,
                     epsilon: Double = 1.0e-6,
                     k: Int = 6) extends AbstractParams[Params]

  def main(args: Array[String]) {
    val defaultParams = Params()

    //Takes care of command line input params and provides in a Params object
    val parser = new OptionParser[Params]("KMeansPrediction") {
      head("KMeansPrediction: a k-means clustering run on Subject Segmentation data.")
      opt[Int]("k")
        .text(s"min number of k-means partitions, default: ${defaultParams.k}")
        .action((x, c) => c.copy(k = x))
      opt[Int]("runs")
        .text(s"number Of Runs , default: ${defaultParams.runs}")
        .action((x, c) => c.copy(runs = x))
      opt[Double]("epsilon")
        .text(s"error margin epsilon, default: ${defaultParams.epsilon}")
        .action((x, c) => c.copy(epsilon = x))
      opt[String]("initializationMode")
        .text(s"initialization mode (${InitializationMode.values.mkString(",")}), " +
          s"default: ${defaultParams.initializationMode}")
        .action((x, c) => c.copy(initializationMode = InitializationMode.withName(x)))
      opt[Long]("seed")
        .text(s"seed, default: Randomly generated by scala.util.Random.nextLong()")
        .action((x, c) => c.copy(seed = x))
      opt[String]("master")
        .text(s"master url, default: ${defaultParams.master}")
        .action((x, c) => c.copy(master = x))
      opt[String]("appName")
        .text(s"application name, default: ${defaultParams.appName}")
        .action((x, c) => c.copy(appName = x))
      arg[String]("inputFile")
        .required()
        .text("input file the subject dataset, required")
        .action((x, c) => c.copy(inputFile = x))
      arg[String]("outputFile")
        .required()
        .text("output file for the run, required")
        .action((x, c) => c.copy(outputFile = x))

      note(
        """
          |For example, the following command runs this app on a synthetic dataset:
          |
          |The input file is of the following format
          |-----------+-------------------------+----------
          |SubjectID | ... feature vectors ... | SegmentID
          |-----------+-------------------------+----------
          |
          |In data cleansing step header, the first and the last columns are dropped, 
          |The feature vectors can be expanded and collapsed as desired but a minimum one is required.
          |
          | bin/spark-submit --class com.wipro.cto.cs.kmeans.KMeansPrediction \
          |  SubjectSegmentation-1.0.0-SNAPSHOT.jar \
          |  --k 5 \
          |  --runs 8 \
          |  --epsilon 1.0e-4 \
          |  --initializationMode Random \
          |  --seed 1000 \
          |  --appName Subject_Segmentation_K-Means\
          |  --master spark://127.0.0.1/master \ 
          |  data/InputData.csv \
          |  data/Subjectkmeans.out
        """.stripMargin)
    }

    parser.parse(args, defaultParams).map { params =>
      run(params)
    } getOrElse {
      System.exit(1)
    }
  }

  def run(params: Params): Unit = {

    //Spark configuration is created
    val conf = new SparkConf().setAppName(params.appName).setMaster(params.master)

    //Spark Context is created
    val sc = new SparkContext(conf)

    //Creates an RDD from the input file
    val rawData = sc.textFile(params.inputFile)

    //Drops the header and the first and last columns
    val cleansedData = getCleansedData(rawData)

    //Extracts feature vectors from labeledpoint
    val featureVectors = cleansedData.map(_.features)

    //Determine the input initialization mode
    val initMode = params.initializationMode match {
      case Random => KMeans.RANDOM
      case Parallel => KMeans.K_MEANS_PARALLEL
    }

    //Set the k-means parameters
    val kmeans = new KMeans()
      .setK(params.k)
      .setRuns(params.runs)
      .setEpsilon(params.epsilon)
      .setInitializationMode(initMode)
      .setSeed(params.seed)

    //Create the model 
    val model = kmeans.run(featureVectors)

    //Predict the clusters from the model
    val newdata = featureVectors.map { datum =>
      val prediction = model.predict(datum)
      formatOutputString(datum, prediction)
    }

    //Save the predictions
    newdata.saveAsTextFile(params.outputFile)

    //Spark Context is stopped
    sc.stop()

  }

}
